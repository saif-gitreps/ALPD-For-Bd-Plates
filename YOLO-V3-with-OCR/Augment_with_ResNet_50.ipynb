{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU8yBHgUrwzN",
        "outputId": "50a4ec80-a787-4bfa-df36-12cecd0c155e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.4.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lUcLVbEo7df",
        "outputId": "531f950a-8af8-4ce9-8f13-1901d3b115c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-3425966744.py:20: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§© (97 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦¹ (37 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:27<00:00,  2.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦¸à¦¿à¦²à§‡à¦Ÿ (5 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:04<00:00, 21.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦² (40 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:29<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦®à§‡à¦Ÿà§à¦°à§‡à¦¾ (63 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:25<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦ª (3 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 24.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦¨ (13 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87/87 [00:09<00:00,  9.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦¬ (7 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:05<00:00, 15.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦®à¦¯à¦¼à¦®à¦¨à¦¸à¦¿à¦‚à¦¹ (1 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:02<00:00, 35.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦¢à¦¾à¦•à¦¾ (88 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:08<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦— (46 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:31<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦˜ (4 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 96/96 [00:03<00:00, 24.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦š (3 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 27.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§® (63 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:24<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦šà¦Ÿà§à¦Ÿ (6 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [00:05<00:00, 18.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦¢ (6 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [00:05<00:00, 17.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦– (2 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 98/98 [00:02<00:00, 36.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à¦• (5 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:04<00:00, 20.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§­ (80 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:13<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§¯ (64 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:22<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§¬ (83 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:10<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§« (82 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:11<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Augmenting class: à§ª (86 images)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:08<00:00,  1.57it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations import (\n",
        "    Rotate, ShiftScaleRotate, RandomBrightnessContrast,\n",
        "    GaussNoise, Affine\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "base_dir = '/content/drive/MyDrive/datasets/test-yolo-2-4-annotations/char_dataset'\n",
        "min_samples = 100  # target number of images per class\n",
        "image_size = 64  # assuming 64x64\n",
        "\n",
        "# Define safe augmentations\n",
        "transform = A.Compose([\n",
        "    A.Rotate(limit=2, p=0.7),  # Â±2 degrees\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
        "    A.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=0, p=0.5)\n",
        "])\n",
        "\n",
        "# Augment underrepresented classes\n",
        "for class_name in os.listdir(base_dir):\n",
        "    class_path = os.path.join(base_dir, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    images = [f for f in os.listdir(class_path) if f.endswith('.png')]\n",
        "    current_count = len(images)\n",
        "\n",
        "    if current_count >= min_samples:\n",
        "        continue\n",
        "\n",
        "    print(f\"ğŸ”„ Augmenting class: {class_name} ({current_count} images)\")\n",
        "\n",
        "    needed = min_samples - current_count\n",
        "    augment_idx = 0\n",
        "\n",
        "    for i in tqdm(range(needed)):\n",
        "        img_name = images[i % current_count]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.resize(image, (image_size, image_size))\n",
        "\n",
        "        augmented = transform(image=image)\n",
        "        aug_image = augmented[\"image\"]\n",
        "\n",
        "        save_name = f\"aug_{augment_idx}_{img_name}\"\n",
        "        cv2.imwrite(os.path.join(class_path, save_name), aug_image)\n",
        "        augment_idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import os\n"
      ],
      "metadata": {
        "id": "i2t_4DFYvqlv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "DATASET_PATH = \"/content/drive/MyDrive/datasets/test-yolo-2-4-annotations/char_dataset\"\n",
        "IMAGE_SIZE = (64, 64)  # ResNet expects at least 32Ã—32, we're okay\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "    DATASET_PATH,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',  # for softmax classification\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    DATASET_PATH,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0iv3NBswsPl",
        "outputId": "6e0e8fad-53d9-4559-f44e-4f62de7addc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2786 files belonging to 26 classes.\n",
            "Using 2229 files for training.\n",
            "Found 2786 files belonging to 26 classes.\n",
            "Using 557 files for validation.\n",
            "Classes: ['à¦•', 'à¦–', 'à¦—', 'à¦˜', 'à¦š', 'à¦šà¦Ÿà§à¦Ÿ', 'à¦¢', 'à¦¢à¦¾à¦•à¦¾', 'à¦¨', 'à¦ª', 'à¦¬', 'à¦®à¦¯à¦¼à¦®à¦¨à¦¸à¦¿à¦‚à¦¹', 'à¦®à§‡à¦Ÿà§à¦°à§‡à¦¾', 'à¦²', 'à¦¸à¦¿à¦²à§‡à¦Ÿ', 'à¦¹', 'à§¦', 'à§§', 'à§¨', 'à§©', 'à§ª', 'à§«', 'à§¬', 'à§­', 'à§®', 'à§¯']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "\n",
        "def preprocess_rgb(image, label):\n",
        "    # Ensure the image has 3 channels before converting to grayscale and then to rgb\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))  # Convert 1 channel â†’ 3\n",
        "    image = preprocess_input(image)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess_rgb).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_rgb).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WE48ZHmJwsnn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ResNet50 with pretrained ImageNet weights\n",
        "base_model = ResNet50(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(64, 64, 3)\n",
        ")\n",
        "base_model.trainable = False  # Freeze all layers initially\n",
        "\n",
        "# Add custom classification head\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJqV02hrw0--",
        "outputId": "2113ae3f-be11-4b76-b761-d585aa8f8951"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=35\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrptY26uxDeI",
        "outputId": "8a28eab0-f173-47fe-f3be-1aa5b7be2f4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 6s/step - accuracy: 0.4160 - loss: 2.6029 - val_accuracy: 0.8564 - val_loss: 0.5923\n",
            "Epoch 2/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 113ms/step - accuracy: 0.8760 - loss: 0.4768 - val_accuracy: 0.8815 - val_loss: 0.4726\n",
            "Epoch 3/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 119ms/step - accuracy: 0.9169 - loss: 0.3178 - val_accuracy: 0.9031 - val_loss: 0.3822\n",
            "Epoch 4/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 109ms/step - accuracy: 0.9456 - loss: 0.1792 - val_accuracy: 0.9192 - val_loss: 0.3194\n",
            "Epoch 5/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 107ms/step - accuracy: 0.9593 - loss: 0.1537 - val_accuracy: 0.9336 - val_loss: 0.2834\n",
            "Epoch 6/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 112ms/step - accuracy: 0.9781 - loss: 0.0966 - val_accuracy: 0.9264 - val_loss: 0.2715\n",
            "Epoch 7/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - accuracy: 0.9779 - loss: 0.0883 - val_accuracy: 0.9318 - val_loss: 0.2769\n",
            "Epoch 8/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 103ms/step - accuracy: 0.9760 - loss: 0.0849 - val_accuracy: 0.9372 - val_loss: 0.2717\n",
            "Epoch 9/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 108ms/step - accuracy: 0.9849 - loss: 0.0586 - val_accuracy: 0.9354 - val_loss: 0.2816\n",
            "Epoch 10/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 113ms/step - accuracy: 0.9866 - loss: 0.0447 - val_accuracy: 0.9390 - val_loss: 0.2803\n",
            "Epoch 11/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 113ms/step - accuracy: 0.9894 - loss: 0.0384 - val_accuracy: 0.9300 - val_loss: 0.2806\n",
            "Epoch 12/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 126ms/step - accuracy: 0.9889 - loss: 0.0424 - val_accuracy: 0.9282 - val_loss: 0.2928\n",
            "Epoch 13/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 126ms/step - accuracy: 0.9926 - loss: 0.0284 - val_accuracy: 0.9336 - val_loss: 0.2959\n",
            "Epoch 14/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.9893 - loss: 0.0373 - val_accuracy: 0.9408 - val_loss: 0.2812\n",
            "Epoch 15/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 96ms/step - accuracy: 0.9915 - loss: 0.0265 - val_accuracy: 0.9372 - val_loss: 0.2951\n",
            "Epoch 16/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 122ms/step - accuracy: 0.9881 - loss: 0.0411 - val_accuracy: 0.9354 - val_loss: 0.3424\n",
            "Epoch 17/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9893 - loss: 0.0332 - val_accuracy: 0.9479 - val_loss: 0.3245\n",
            "Epoch 18/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 128ms/step - accuracy: 0.9898 - loss: 0.0305 - val_accuracy: 0.9425 - val_loss: 0.3421\n",
            "Epoch 19/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 122ms/step - accuracy: 0.9843 - loss: 0.0405 - val_accuracy: 0.9228 - val_loss: 0.4214\n",
            "Epoch 20/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.9845 - loss: 0.0529 - val_accuracy: 0.9264 - val_loss: 0.4301\n",
            "Epoch 21/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 111ms/step - accuracy: 0.9745 - loss: 0.0756 - val_accuracy: 0.9192 - val_loss: 0.3760\n",
            "Epoch 22/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 103ms/step - accuracy: 0.9812 - loss: 0.0548 - val_accuracy: 0.9425 - val_loss: 0.3179\n",
            "Epoch 23/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.9919 - loss: 0.0271 - val_accuracy: 0.9390 - val_loss: 0.3349\n",
            "Epoch 24/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.9873 - loss: 0.0439 - val_accuracy: 0.9372 - val_loss: 0.3617\n",
            "Epoch 25/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 108ms/step - accuracy: 0.9919 - loss: 0.0276 - val_accuracy: 0.9354 - val_loss: 0.3303\n",
            "Epoch 26/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 114ms/step - accuracy: 0.9953 - loss: 0.0171 - val_accuracy: 0.9408 - val_loss: 0.3093\n",
            "Epoch 27/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 111ms/step - accuracy: 0.9945 - loss: 0.0176 - val_accuracy: 0.9425 - val_loss: 0.3265\n",
            "Epoch 28/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 8s/step - accuracy: 0.9895 - loss: 0.0243 - val_accuracy: 0.9479 - val_loss: 0.3177\n",
            "Epoch 29/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 122ms/step - accuracy: 0.9943 - loss: 0.0155 - val_accuracy: 0.9425 - val_loss: 0.3632\n",
            "Epoch 30/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 119ms/step - accuracy: 0.9933 - loss: 0.0228 - val_accuracy: 0.9390 - val_loss: 0.3964\n",
            "Epoch 31/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.9896 - loss: 0.0281 - val_accuracy: 0.9282 - val_loss: 0.4471\n",
            "Epoch 32/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.9966 - loss: 0.0193 - val_accuracy: 0.9443 - val_loss: 0.4111\n",
            "Epoch 33/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 114ms/step - accuracy: 0.9900 - loss: 0.0306 - val_accuracy: 0.9408 - val_loss: 0.3738\n",
            "Epoch 34/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9868 - loss: 0.0423 - val_accuracy: 0.9102 - val_loss: 0.5729\n",
            "Epoch 35/35\n",
            "\u001b[1m70/70\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 108ms/step - accuracy: 0.9829 - loss: 0.0924 - val_accuracy: 0.9336 - val_loss: 0.5050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(val_ds)\n",
        "print(f\"Validation Accuracy: {acc:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwjQ-2ZIxeU9",
        "outputId": "10defdfe-9a2e-4582-ffc9-813804a974c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9221 - loss: 0.5151\n",
            "Validation Accuracy: 93.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/datasets/test-yolo-2-4-annotations/bangla_ocr_resnet50_v1.keras\")"
      ],
      "metadata": {
        "id": "httEjkaNxJdL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwwWdgA4AKza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}